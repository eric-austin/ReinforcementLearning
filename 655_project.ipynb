{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"655_project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yRaKw0bkqf2_"},"source":["# Import Statements\n"]},{"cell_type":"code","metadata":{"id":"lsRSNuVtqf2_"},"source":["import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1HeHkvpqf2_"},"source":["# Agents "]},{"cell_type":"markdown","metadata":{"id":"88MnLeYMWRFm"},"source":["## Sarsa agent"]},{"cell_type":"code","metadata":{"id":"uS5Z4daYWate"},"source":["class SarsaAgent():\n","    \"\"\"Class for a Sarsa agent\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Leave agent initialization for agent_init method\"\"\"\n","\n","        self.last_action = None\n","        self.last_state = None\n","        self.epsilon = None\n","        self.target_epsilon = None\n","        self.gamma = None\n","        self.w = None\n","        self.alpha = None\n","        self.initial_weights = None\n","        self.num_actions = None\n","        self.features = None\n","        self.terminate = False\n","        self.dimensions = 0\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Set up the agent at the beginning of the experiment\"\"\"\n","\n","        self.epsilon = agent_info.get(\"epsilon\", 0.3)\n","        self.target_epsilon = agent_info.get(\"target_epsilon\", 0.3)\n","        self.gamma = agent_info.get(\"gamma\", 1.0)\n","        self.alpha = agent_info.get(\"alpha\", 0.1)\n","        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n","        self.num_states = agent_info.get(\"num_states\", 4)\n","        self.num_actions = agent_info.get(\"num_actions\", 2)\n","        self.dimensions = agent_info.get(\"dimensions\", 2)\n","        self.features = agent_info.get(\"features\", np.array(\n","            [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]]))\n","\n","        self.is_testing = agent_info.get(\"is_testing\", False)\n","\n","        self.w = np.ones((self.dimensions,)) * self.initial_weights\n","\n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","\n","        top = float(\"-inf\")\n","        ties = []\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return np.random.choice(ties)\n","\n","    def select_action(self, state):\n","        \"\"\"\n","        Selects an action using epsilon greedy\n","        Args:\n","        state - int, the current state\n","        Returns:\n","        (chosen_action, max_action_value) - (int, float), tuple of the chosen action\n","                                        (epsilon greedy behaviour) and the max action value (target update)\n","        \"\"\"\n","\n","        action_values = []\n","        chosen_action = None\n","        values = np.dot(self.features[state], self.w)\n","        action_values = values.tolist()\n","        for index, item in enumerate(action_values):\n","            if math.isnan(action_values[index]):\n","                action_values[index] = 0\n","                self.w = np.zeros(self.w.shape)\n","\n","        if self.is_testing:\n","            chosen_action = self.argmax(action_values)\n","        else:\n","\n","            if np.random.random() < self.epsilon:\n","                chosen_action = np.random.choice(self.num_actions)\n","            else:\n","                chosen_action = self.argmax(action_values)\n","        # ----------------\n","\n","        return chosen_action, action_values[chosen_action]\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","        current_action, _ = self.select_action(state)\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","\n","        # select action based on new state\n","        current_action, epsilon_q_val = self.select_action(state)\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w)\n","        # perform update to weights\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward + self.gamma * epsilon_q_val - last_q_val) * self.features[\n","                self.last_state, self.last_action]\n","        # update previous state, action\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w)\n","        # perform the last update\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward - last_q_val) * self.features[self.last_state, self.last_action]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1RK1JfwAqf2_"},"source":["## Q-Learning Agent"]},{"cell_type":"code","metadata":{"id":"Hc1I7G60qf3A"},"source":["class QLAgent():\n","    \"\"\"Class for a Q-learning agent\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Leave agent initialization for agent_init method\"\"\"\n","\n","        self.last_action = None\n","        self.last_state = None\n","        self.epsilon = None\n","        self.gamma = None\n","        self.w = None\n","        self.alpha = None\n","        self.initial_weights = None\n","        self.num_actions = None\n","        self.features = None\n","        self.terminate = False\n","        self.dimensions = 0\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Set up the agent at the beginning of the experiment\"\"\"\n","\n","        self.epsilon = agent_info.get(\"epsilon\", 0.3)\n","        self.gamma = agent_info.get(\"gamma\", 1.0)\n","        self.alpha = agent_info.get(\"alpha\", 0.1)\n","        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n","        self.num_states = agent_info.get(\"num_states\", 4)\n","        self.num_actions = agent_info.get(\"num_actions\", 2)\n","        self.dimensions = agent_info.get(\"dimensions\", 2)\n","        self.features = agent_info.get(\"features\", np.array(\n","            [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]]))\n","\n","        self.is_testing = agent_info.get(\"is_testing\", False)\n","\n","        self.w = np.ones((self.dimensions,)) * self.initial_weights\n","\n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","\n","        top = float(\"-inf\")\n","        ties = []\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return np.random.choice(ties)\n","\n","    def select_action(self, state):\n","        \"\"\"\n","        Selects an action using epsilon greedy\n","        Args:\n","        state - int, the current state\n","        Returns:\n","        (chosen_action, max_action_value) - (int, float), tuple of the chosen action\n","                                        (epsilon greedy behaviour) and the max action value (target update)\n","        \"\"\"\n","\n","        action_values = []\n","        chosen_action = None\n","        values = np.dot(self.features[state], self.w)\n","        action_values = values.tolist()\n","        for index, item in enumerate(action_values):\n","            if math.isnan(action_values[index]):\n","                action_values[index] = 0\n","                self.w = np.zeros(self.w.shape)\n","\n","        if self.is_testing:\n","            chosen_action = self.argmax(action_values)\n","        else:\n","            if np.random.random() < self.epsilon:\n","                chosen_action = np.random.choice(self.num_actions)\n","            else:\n","                chosen_action = self.argmax(action_values)\n","\n","        return chosen_action, max(action_values)\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","        current_action, _ = self.select_action(state)\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","\n","        # find max action value based on new state\n","        _, max_q_val = self.select_action(state)\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w)\n","        # perform update to weights\n","\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward + self.gamma * max_q_val - last_q_val) * self.features[\n","                self.last_state, self.last_action]\n","\n","        current_action, _ = self.select_action(state)\n","\n","\n","        # update previous state, action\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w)\n","        # perform the last update\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward - last_q_val) * self.features[self.last_state, self.last_action]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9km5sWokcjzY"},"source":["## Expected Sarsa"]},{"cell_type":"code","metadata":{"id":"S6AnzlWxcuCs"},"source":["class ExAgent():\n","    \"\"\"Class for a Q-learning agent\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Leave agent initialization for agent_init method\"\"\"\n","\n","        self.last_action = None\n","        self.last_state = None\n","        self.epsilon = None\n","        self.target_epsilon = None\n","        self.gamma = None\n","        self.w = None\n","        self.alpha = None\n","        self.initial_weights = None\n","        self.num_actions = None\n","        self.features = None\n","        self.terminate = False\n","        self.dimensions = 0\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Set up the agent at the beginning of the experiment\"\"\"\n","\n","        self.epsilon = agent_info.get(\"epsilon\", 0.3)\n","        self.target_epsilon = agent_info.get(\"target_epsilon\", 0.3)\n","        self.gamma = agent_info.get(\"gamma\", 1.0)\n","        self.alpha = agent_info.get(\"alpha\", 0.1)\n","        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n","        self.num_states = agent_info.get(\"num_states\", 4)\n","        self.num_actions = agent_info.get(\"num_actions\", 2)\n","        self.dimensions = agent_info.get(\"dimensions\", 2)\n","        self.features = agent_info.get(\"features\", np.array(\n","            [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]]))\n","        self.is_testing = agent_info.get(\"is_testing\", False)\n","\n","        self.w = np.ones((self.dimensions,)) * self.initial_weights\n","\n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","\n","        top = float(\"-inf\")\n","        ties = []\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return np.random.choice(ties)\n","\n","    def select_action(self, state):\n","        \"\"\"\n","        Selects an action using epsilon greedy\n","        Args:\n","        state - int, the current state\n","        Returns:\n","        (chosen_action, max_action_value) - (int, float), tuple of the chosen action\n","                                        (epsilon greedy behaviour) and the max action value (target update)\n","        \"\"\"\n","\n","        action_values = []\n","        chosen_action = None\n","        values = np.dot(self.features[state], self.w)\n","        action_values = values.tolist()\n","\n","        for index, item in enumerate(action_values):\n","            if math.isnan(action_values[index]):\n","                action_values[index] = 0\n","                self.w = np.zeros(self.w.shape)\n","\n","        if self.is_testing:\n","            chosen_action = self.argmax(action_values)\n","        else:\n","            if np.random.random() < self.epsilon:\n","                chosen_action = np.random.choice(self.num_actions)\n","            else:\n","                chosen_action = self.argmax(action_values)\n","\n","        max_value = action_values[self.argmax(action_values)]\n","        max_weight = 1 - self.target_epsilon + (self.target_epsilon / self.num_actions)\n","        weight = self.target_epsilon / self.num_actions\n","        expected_value = weight * np.sum(action_values) - weight * max_value + max_weight * max_value\n","\n","        return chosen_action, expected_value\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","        current_action, _ = self.select_action(state)\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","\n","        # find max action value based on new state\n","        _, expected_value = self.select_action(state)\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w)\n","        # perform update to weights\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward + self.gamma * expected_value - last_q_val) * self.features[\n","                self.last_state, self.last_action]\n","\n","        ####\n","        current_action, _ = self.select_action(state)\n","        ####\n","\n","        # update previous state, action\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w)\n","        # perform the last update\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward - last_q_val) * self.features[self.last_state, self.last_action]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqTKgk8pb-iJ"},"source":["## Double Q Agent"]},{"cell_type":"code","metadata":{"id":"oegE_-9Lb9lo"},"source":["class DoubleQLAgent():\n","    \"\"\"Class for a Double Q-learning agent\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Leave agent initialization for agent_init method\"\"\"\n","\n","        self.last_action = None\n","        self.last_state = None\n","        self.epsilon = None\n","        self.gamma = None\n","        self.w1 = None\n","        self.w2 = None\n","        self.alpha = None\n","        self.initial_weights = None\n","        self.num_actions = None\n","        self.features = None\n","        self.terminate = False\n","        self.dimensions = 0\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Set up the agent at the beginning of the experiment\"\"\"\n","\n","        self.epsilon = agent_info.get(\"epsilon\", 0.3)\n","        self.gamma = agent_info.get(\"gamma\", 1.0)\n","        self.alpha = agent_info.get(\"alpha\", 0.1)\n","        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n","        self.num_states = agent_info.get(\"num_states\", 4)\n","        self.num_actions = agent_info.get(\"num_actions\", 2)\n","        self.dimensions = agent_info.get(\"dimensions\", 2)\n","        self.features = agent_info.get(\"features\", np.array(\n","            [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]]))\n","        self.is_testing = agent_info.get(\"is_testing\", False)\n","\n","        self.w1 = np.ones((self.dimensions,)) * self.initial_weights\n","        self.w2 = np.ones((self.dimensions,)) * self.initial_weights\n","\n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","\n","        top = float(\"-inf\")\n","        ties = []\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return np.random.choice(ties)\n","\n","    def select_action(self, state, q_to_use):\n","        \"\"\"\n","        Selects an action using epsilon greedy using both value estimations\n","        Args:\n","        state - int, the current state\n","        Returns:\n","        (chosen_action, max_action_value) - (int, float), tuple of the chosen action\n","                                        (epsilon greedy behaviour)\n","                                        and the max action value (target update)\n","        \"\"\"\n","\n","        action_values = []\n","        chosen_action = None\n","        # get state-action values for both estimators q1 and q1\n","        values1 = np.dot(self.features[state], self.w1)\n","        values2 = np.dot(self.features[state], self.w2)\n","        values = values1 + values2\n","        action_values = values.tolist()\n","\n","        for index, item in enumerate(action_values):\n","            if math.isnan(action_values[index]):\n","                action_values[index] = 0\n","                self.w = np.zeros(self.w.shape)\n","\n","        # select action from combined q values\n","        if self.is_testing:\n","            chosen_action = self.argmax(action_values)\n","        else:\n","            if np.random.random() < self.epsilon:\n","                chosen_action = np.random.choice(self.num_actions)\n","            else:\n","                chosen_action = self.argmax(action_values)\n","        # get max estimate from q_to_use\n","        if q_to_use == 1:\n","            max_val = np.dot(self.features[state][self.argmax(values1.tolist())], self.w2)\n","        else:\n","            max_val = np.dot(self.features[state][self.argmax(values2.tolist())], self.w1)\n","        # ----------------\n","\n","        return chosen_action, max_val\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","        current_action, _ = self.select_action(state, 1)\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","\n","        # randomly select which q_value to use for max estimate\n","        q_to_use = np.random.randint(1, 3)\n","        # find max action value based on new state\n","        _, max_q_val = self.select_action(state, q_to_use)\n","        # update based on q_to_use\n","        if not self.is_testing:\n","            if q_to_use == 1:\n","                # get q hat value for previous state-action pair\n","                last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w2)\n","                # perform update to weights\n","                self.w2 += self.alpha * (reward + self.gamma * max_q_val - last_q_val) * self.features[\n","                    self.last_state, self.last_action]\n","            else:\n","                # get q hat value for previous state-action pair\n","                last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w1)\n","                # perform update to weights\n","                self.w1 += self.alpha * (reward + self.gamma * max_q_val - last_q_val) * self.features[\n","                    self.last_state, self.last_action]\n","        ####\n","        current_action, _ = self.select_action(state, q_to_use)\n","        ####\n","\n","        # update previous state, action\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        # randomly select which q_value to use for max estimate\n","        q_to_use = np.random.randint(1, 3)\n","        if not self.is_testing:\n","            if q_to_use == 1:\n","                # get q hat value for previous state-action pair\n","                last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w2)\n","                # perform the last update\n","                self.w2 += self.alpha * (reward - last_q_val) * self.features[self.last_state, self.last_action]\n","            else:\n","                # get q hat value for previous state-action pair\n","                last_q_val = np.dot(self.features[self.last_state, self.last_action], self.w1)\n","                # perform the last update\n","                self.w1 += self.alpha * (reward - last_q_val) * self.features[self.last_state, self.last_action]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AcSMp5i_pmlm"},"source":["## Dyna Q Agent"]},{"cell_type":"code","metadata":{"id":"pL1-P2kupuwL"},"source":["class DynaQAgent():\n","    \"\"\"Class for a Q-learning agent\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Leave agent initialization for agent_init method\"\"\"\n","\n","        self.past_action = None\n","        self.past_state = None\n","        self.epsilon = None\n","        self.gamma = None\n","        self.w = None\n","        self.alpha = None\n","        self.initial_weights = None\n","        self.num_actions = None\n","        self.features = None\n","        self.terminate = False\n","        self.dimensions = 0\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Set up the agent at the beginning of the experiment\"\"\"\n","\n","        self.epsilon = agent_info.get(\"epsilon\", 0.3)\n","        self.gamma = agent_info.get(\"gamma\", 1.0)\n","        self.alpha = agent_info.get(\"alpha\", 0.1)\n","        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n","        self.num_states = agent_info.get(\"num_states\", 4)\n","        self.num_actions = agent_info.get(\"num_actions\", 2)\n","        self.dimensions = agent_info.get(\"dimensions\", 2)\n","        self.features = agent_info.get(\"features\", np.array(\n","            [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]]))\n","        self.w = np.ones((self.dimensions,)) * self.initial_weights\n","\n","        self.is_testing = agent_info.get(\"is_testing\", False)\n","\n","        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n","        self.terminal_state = agent_info.get(\"terminal_state\", 4)\n","\n","        self.past_action = -1\n","        self.past_state = -1\n","        self.model = {}\n","\n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","\n","        top = float(\"-inf\")\n","        ties = []\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return np.random.choice(ties)\n","\n","    def select_action(self, state):\n","        \"\"\"\n","        Selects an action using epsilon greedy\n","        Args:\n","        state - int, the current state\n","        Returns:\n","        (chosen_action, max_action_value) - (int, float), tuple of the chosen action\n","                                        (epsilon greedy behaviour) and the max action value (target update)\n","        \"\"\"\n","\n","        action_values = []\n","        chosen_action = None\n","\n","        values = np.dot(self.features[state], self.w)\n","\n","        action_values = values.tolist()\n","\n","        for index, item in enumerate(action_values):\n","            if math.isnan(action_values[index]):\n","                action_values[index] = 0\n","                self.w = np.zeros(self.w.shape)\n","\n","        if self.is_testing:\n","            chosen_action = self.argmax(action_values)\n","        else:\n","            if np.random.random() < self.epsilon:\n","                chosen_action = np.random.choice(self.num_actions)\n","            else:\n","                chosen_action = self.argmax(action_values)\n","        # ----------------\n","\n","        return chosen_action, max(action_values)\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","        current_action, _ = self.select_action(state)\n","        self.past_state = state\n","        self.past_action = current_action\n","        return current_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","\n","        # find max action value based on new state\n","        _, max_q_val = self.select_action(state)\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.past_state, self.past_action], self.w)\n","        # perform update to weights\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward + self.gamma * max_q_val - last_q_val) * self.features[\n","                self.past_state, self.past_action]\n","\n","            self.update_model(self.past_state, self.past_action, state, reward)\n","            self.planning_step()\n","        ####\n","        current_action, _ = self.select_action(state)\n","        ####\n","\n","        # update previous state, action\n","        self.past_state = state\n","        self.past_action = current_action\n","        return current_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        # get q hat value for previous state-action pair\n","        last_q_val = np.dot(self.features[self.past_state, self.past_action], self.w)\n","        # perform the last update\n","        if not self.is_testing:\n","            self.w += self.alpha * (reward - last_q_val) * self.features[self.past_state, self.past_action]\n","            self.update_model(self.past_state, self.past_action, self.terminal_state, reward)\n","            self.planning_step()\n","\n","    def update_model(self, past_state, past_action, state, reward):\n","\n","        # Update the model with the (s,a,s',r) tuple (1~4 lines)\n","\n","        # ----------------\n","        # your code here\n","\n","        if past_state not in self.model:\n","            self.model[past_state] = {}\n","\n","        self.model[past_state][past_action] = (state, reward)\n","\n","    def planning_step(self):\n","\n","        # your code here\n","        for i in range(self.planning_steps):\n","\n","            state = np.random.choice(list(self.model.keys()))\n","            action = np.random.choice(list(self.model[state].keys()))\n","\n","            next_state, next_reward = self.model[state][action]\n","\n","            if next_state != self.terminal_state:\n","                _, max_value = self.select_action(next_state)\n","                self.w += self.alpha * (next_reward + self.gamma * max_value - np.dot(self.features[state][action],self.w)) * self.features[state, action]\n","            else:\n","                self.w += self.alpha * (next_reward - np.dot(self.features[state][action],self.w)) * self.features[state, action]\n","\n","        # ----------------\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uv6eDHZJBC74"},"source":["## Q-learning tablur"]},{"cell_type":"code","metadata":{"id":"AtkTTiq5BI99"},"source":["class QLNFAgent():\n","    \"\"\"Class for a Q-learning agent\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Leave agent initialization for agent_init method\"\"\"\n","\n","        self.last_action = None\n","        self.last_state = None\n","        self.epsilon = None\n","        self.gamma = None\n","        self.w = None\n","        self.alpha = None\n","        self.initial_weights = None\n","        self.num_actions = None\n","        self.features = None\n","        self.terminate = False\n","        self.dimensions = 0\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Set up the agent at the beginning of the experiment\"\"\"\n","\n","        self.epsilon = agent_info.get(\"epsilon\", 0.3)\n","        self.gamma = agent_info.get(\"gamma\", 1.0)\n","        self.alpha = agent_info.get(\"alpha\", 0.1)\n","        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n","        self.num_states = agent_info.get(\"num_states\", 4)\n","        self.num_actions = agent_info.get(\"num_actions\", 2)\n","        self.dimensions = agent_info.get(\"dimensions\", 2)\n","\n","        self.is_testing = agent_info.get(\"is_testing\", False)\n","\n","        self.q = np.zeros((self.num_states, self.num_actions))\n","\n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","\n","        top = float(\"-inf\")\n","        ties = []\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return np.random.choice(ties)\n","\n","    def select_action(self, state):\n","        \"\"\"\n","        Selects an action using epsilon greedy\n","        Args:\n","        state - int, the current state\n","        Returns:\n","        (chosen_action, max_action_value) - (int, float), tuple of the chosen action\n","                                        (epsilon greedy behaviour) and the max action value (target update)\n","        \"\"\"\n","\n","        action_values = []\n","        chosen_action = None\n","        action_values = self.q[state]\n","\n","        for index, item in enumerate(action_values):\n","            if math.isnan(action_values[index]):\n","                action_values[index] = 0\n","\n","        if self.is_testing:\n","            chosen_action = self.argmax(action_values)\n","        else:\n","            if np.random.random() < self.epsilon:\n","                chosen_action = np.random.choice(self.num_actions)\n","            else:\n","                chosen_action = self.argmax(action_values)\n","\n","        return chosen_action, max(action_values)\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","        current_action, _ = self.select_action(state)\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","\n","        # find max action value based on new state\n","        _, max_q_val = self.select_action(state)\n","        # get q hat value for previous state-action pair\n","        last_q_val = self.q[self.last_state, self.last_action]\n","        # perform update to weights\n","\n","        if not self.is_testing:\n","            self.q[self.last_state, self.last_action] = self.q[self.last_state, self.last_action] + self.alpha * (\n","                    reward + self.gamma * max_q_val - last_q_val)\n","\n","        current_action, _ = self.select_action(state)\n","\n","        # update previous state, action\n","        self.last_state = state\n","        self.last_action = current_action\n","        return current_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        # get q hat value for previous state-action pair\n","        last_q_val = self.q[self.last_state, self.last_action]\n","        # perform the last update\n","        if not self.is_testing:\n","            self.q[self.last_state, self.last_action] = self.q[self.last_state, self.last_action] + self.alpha * (\n","                        reward - last_q_val)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVRtrZyNqf3A"},"source":["# Environments"]},{"cell_type":"markdown","metadata":{"id":"hhUjwOmFqf3A"},"source":["## 1 x 5 Gridworld"]},{"cell_type":"code","metadata":{"id":"Gqy6hBLPqf3A"},"source":["class SmallGridworld():\n","    \"\"\"Class for our small 1x5 gridworld for diagnosing delusional bias\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Leave env initialization for env_init method\"\"\"\n","\n","        self.reward_state_term = None\n","        self.grid_h = None\n","        self.grid_w = None\n","        self.start_loc = None\n","        self.goal_loc = None\n","        self.agent_loc = None\n","\n","    def state(self, loc):\n","        \"\"\"Helper method for obtaining single state index from x,y coordinates in gridworld\n","        Args:\n","            loc (int, int): tuple of x,y coordinates in the gridworld\n","        Returns:\n","            index (int): the index of the state corresponding to the coordinates\n","        \"\"\"\n","\n","        x, y = loc\n","        index = x * self.grid_w + y\n","        return index\n","\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","        Note:\n","            Initialize a tuple with the reward, first state, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","        # will initialize these to proper values in env_start()\n","        reward = None\n","        state = None\n","        termination = None\n","        self.reward_state_term = (reward, state, termination)\n","        # default 1x5 world\n","        self.grid_h = env_info.get(\"grid_height\", 1)\n","        self.grid_w = env_info.get(\"grid_width\", 5)\n","        # start at far left\n","        self.start_loc = (0, 0)\n","        # terminal state is far right\n","        self.goal_loc = (0, self.grid_w - 1)\n","\n","    def env_start(self):\n","        \"\"\"The first method called when the episode starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state from the environment.\n","        \"\"\"\n","\n","        reward = 0\n","        self.agent_loc = self.start_loc\n","        state = self.state(self.agent_loc)\n","        termination = False\n","        self.reward_state_term = (reward, state, termination)\n","        return self.reward_state_term[1]\n","\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","        x, y = self.agent_loc\n","        reward = 0\n","        terminal = False\n","\n","        # if we take action 0 in any state we terminate without reward\n","        # except for state 0 which has a 0.1 prob of transitioning to state 3\n","        # and a 0.9 prob of transitioning to terminal state, both with reward = 0.3\n","        if action == 0:\n","            if self.state(self.agent_loc) == 0:\n","                reward += 0.3\n","                if np.random.random() < 0.1:\n","                    y = 3\n","                else:\n","                    terminal = True\n","                    y = 4\n","            else:\n","                terminal = True\n","                y = 4\n","        # if we take action 1 we simply move one state to the right with 0 reward\n","        # except for state 3 which gets reward 2\n","        else:\n","            if self.state(self.agent_loc) == 3:\n","                reward += 2\n","            y = y + 1\n","\n","            if y == 4:\n","              terminal = True\n","\n","        # update agent location\n","\n","        if not self.isInBounds(x, y, self.grid_w, self.grid_h):\n","            x, y = self.agent_loc\n","\n","        self.agent_loc = (x, y)\n","\n","        # check whether state is terminal\n","        if self.state(self.agent_loc) == self.state(self.goal_loc):\n","            terminal = True\n","\n","        self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n","        return self.reward_state_term\n","\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","        self.agent_loc = self.start_loc\n","\n","    def isInBounds(self, x, y, width, height):\n","        # your code here\n","        if x < 0 or x >= height or y < 0 or y >= width:\n","            return False\n","        else:\n","            return True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7b-0ROmDqf3A"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xklUox-ts5FE"},"source":["## 6 x 6 Gridworld"]},{"cell_type":"code","metadata":{"id":"ju6A6SqjtDOK"},"source":["class GridWorldEnvironment():\n","    def env_init(self, env_info):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","                Note:\n","                    Initialize a tuple with the reward, first state, boolean\n","                    indicating if it's terminal.\n","                \"\"\"\n","\n","\n","        # AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably\n","        # used with the term \"state\" for our purposes and for this assignment in particular.\n","        # A difference arises in the use of the terms when we have what is called Partial Observability where\n","        # the environment may return states that may not fully represent all the information needed to\n","        # predict values or make decisions (i.e., the environment is non-Markovian.)\n","\n","        # Set the default height to 4 and width to 12 (as in the diagram given above)\n","        self.grid_h = env_info.get(\"grid_height\", 6)\n","        self.grid_w = env_info.get(\"grid_width\", 6)\n","\n","        # Now, we can define a frame of reference. Let positive x be towards the direction down and\n","        # positive y be towards the direction right (following the row-major NumPy convention.)\n","        # Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1\n","        # and max y is then grid_w - 1. So, we have:\n","        # Starting location of agent is the bottom-left corner, (max x, min y).\n","        self.start_loc = (0, 0)\n","        # Goal location is the bottom-right corner. (max x, max y).\n","        self.goal_loc = (self.grid_h - 1, self.grid_w - 1)\n","\n","        # The cliff will contain all the cells between the start_loc and goal_loc.\n","        # self.cliff = [(self.grid_h - 1, i) for i in range(1, (self.grid_w - 1))]\n","\n","        # Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to\n","        # verify that your understanding of the above code is correct for the default case, i.e., where\n","        # height = 4 and width = 12.\n","\n","    def env_start(self):\n","        \"\"\"The first method called when the episode starts, called before the\n","            agent starts.\n","\n","            Returns:\n","                The first state from the environment.\n","            \"\"\"\n","        reward = 0\n","        # agent_loc will hold the current location of the agent\n","        self.agent_loc = self.start_loc\n","        # state is the one dimensional state representation of the agent location.\n","        state = self.state(self.agent_loc)\n","        termination = False\n","        self.reward_state_term = (reward, state, termination)\n","\n","        return self.reward_state_term[1]\n","\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","            Args:\n","                action: The action taken by the agent\n","\n","            Returns:\n","                (float, state, Boolean): a tuple of the reward, state,\n","                    and boolean indicating if it's terminal.\n","            \"\"\"\n","\n","        x, y = self.agent_loc\n","\n","        # UP\n","        if action == 0:\n","            # Hint: Look at the code given for the other actions and think about the logic in them.\n","            # your code here\n","            x = x - 1\n","\n","\n","        # LEFT\n","        elif action == 1:\n","            y = y - 1\n","\n","        # DOWN\n","        elif action == 2:\n","            x = x + 1\n","\n","        # RIGHT\n","        elif action == 3:\n","            y = y + 1\n","\n","        # Uh-oh\n","        else:\n","            raise Exception(str(action) + \" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!\")\n","\n","        # if the action takes the agent out-of-bounds\n","        # then the agent stays in the same state\n","        if not self.isInBounds(x, y, self.grid_w, self.grid_h):\n","            x, y = self.agent_loc\n","\n","        # assign the new location to the environment object\n","        self.agent_loc = (x, y)\n","\n","        # assign the reward and terminal variables\n","        # - if the agent falls off the cliff (don't forget to reset agent location!)\n","        # - if the agent reaches the goal state\n","        # your code here\n","        if x == self.grid_h - 1 and y == self.grid_w - 1:\n","            reward = 10\n","            terminal = False\n","        elif x == 0 and y == self.grid_w - 1:\n","            reward = 1\n","            terminal = False\n","        elif x == self.grid_h - 1 and y == 0:\n","            reward = 2\n","            terminal = False\n","        else:\n","            reward = 0\n","            terminal = False\n","\n","        self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n","        return self.reward_state_term\n","\n","    def env_end(self, reward):\n","        raise NotImplementedError\n","\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","        self.agent_loc = self.start_loc\n","\n","    # helper method\n","    def state(self, loc):\n","        # your code here\n","        index = self.grid_w * loc[0] + loc[1]\n","        return index\n","\n","    def isInBounds(self, x, y, width, height):\n","        # your code here\n","        if x < 0 or x >= height or y < 0 or y >= width:\n","            return False\n","        # elif x == 2 and y >= 2:\n","        #     return False\n","        else:\n","            return True\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWF_wATQHc_z"},"source":["# Run environment"]},{"cell_type":"markdown","metadata":{"id":"FsdKWQ__G7FL"},"source":["##First set of experiments"]},{"cell_type":"markdown","metadata":{"id":"PpU4En9gG_XQ"},"source":["### Sarsa w/ epsilon = 0.7"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26o2UL5vHOvt","executionInfo":{"elapsed":37152,"status":"ok","timestamp":1607469864422,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"a86658a7-d91e-418e-8844-ea4151bb8797"},"source":["# run sarsa on first environment\n","# epsilon = 0.7\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.1, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = SarsaAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.4673285   0.31296858]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LHLXpGr_OJzp"},"source":["### Sarsa w/ epsilon = 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XAZc_w3bOOS7","executionInfo":{"elapsed":30068,"status":"ok","timestamp":1607469909586,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"4d602ef8-f013-49d1-c653-dd25f4868d63"},"source":["# run sarsa on first environment\n","# epsilon = 0.1\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.1, \"target_epsilon\": 0.1, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = SarsaAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","sarsa_0_1 = np.mean(np.array(average_weight), 0)\n","print(sarsa_0_1)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(sarsa_0_1)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(sarsa_0_1)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-1.45498804  0.43011628]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NNz74cChOhal"},"source":["### On-policy expected Sarsa w/ epsilon = 0.7"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2iawMWnOptl","executionInfo":{"elapsed":96229,"status":"ok","timestamp":1607470028245,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"5e3aa86d-d533-4b2d-a2e6-b8d34f80f7a6"},"source":["# run expected sarsa on first environment (on policy)\n","# epsilon = 0.7\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.7, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.4794075   0.31769861]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uV4lM2afPTTU"},"source":["### On-policy expected Sarsa w/ epsilon = 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-YUTYHCEPYt6","executionInfo":{"elapsed":61498,"status":"ok","timestamp":1607470115401,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"da3526a9-acfb-4110-b82f-9c39c7fe6148"},"source":["# run expected sarsa on first environment (on policy)\n","# epsilon = 0.1\n","# average over 10 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.1, \"target_epsilon\": 0.1, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_1 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_1)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_1)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_1)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-1.45853638  0.43953156]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ePgvDyIyQDp1"},"source":["### Q-learning w/ epsilon = 0.7"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A4Aw6rYUQIma","executionInfo":{"elapsed":44858,"status":"ok","timestamp":1607470172273,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"d725e9ed-5403-40a6-999d-cbf2d87451db"},"source":["# run q-learning on first environment\n","# epsilon = 0.7\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.7, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = QLAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","q_0_7 = np.mean(np.array(average_weight), 0)\n","print(q_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(q_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(q_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.29218259  0.29843361]\n","Action in state 1: 1\n","Action in state 4: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZyCgCOPGSj9B"},"source":["### Q-learning w/ epsilon = 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdeecDmoSoGp","executionInfo":{"elapsed":32226,"status":"ok","timestamp":1607470214848,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"a13d9245-a6e5-4ff2-897c-ef418f15e55a"},"source":["# run q-learning on first environment\n","# epsilon = 0.1\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.1, \"target_epsilon\": 0.7, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = QLAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","q_0_7 = np.mean(np.array(average_weight), 0)\n","print(q_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(q_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(q_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-1.44675961  0.45675284]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UMeE3Xa5TU4m"},"source":["### Double Q w/ epsilon = 0.7"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTBjvBiFTZsY","executionInfo":{"elapsed":82366,"status":"ok","timestamp":1607470321319,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"328e1408-98c3-4988-b276-af8464a8eed5"},"source":["# run double q-learning on first environment\n","# epsilon = 0.7\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.7, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = DoubleQLAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vectors to the list of learned weights\n","    average_weight.append(agent.w1)\n","    average_weight.append(agent.w2)\n","\n","#all runs have ended, get average of weights learned on all runs\n","dbl_q_0_7 = np.mean(np.array(average_weight), 0)\n","print(dbl_q_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(dbl_q_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(dbl_q_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.28382887  0.29459302]\n","Action in state 1: 1\n","Action in state 4: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hdd9ZIXNVNE0"},"source":["### Double Q w/ epsilon = 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhugiIo0VShY","executionInfo":{"elapsed":52927,"status":"ok","timestamp":1607470392859,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"5d550f0d-90ce-436f-8258-9a9b5c4659db"},"source":["# run double q-learning on first environment\n","# epsilon = 0.1\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.1, \"target_epsilon\": 0.7, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = DoubleQLAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vectors to the list of learned weights\n","    average_weight.append(agent.w1)\n","    average_weight.append(agent.w2)\n","\n","#all runs have ended, get average of weights learned on all runs\n","dbl_q_0_1 = np.mean(np.array(average_weight), 0)\n","print(dbl_q_0_1)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(dbl_q_0_1)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(dbl_q_0_1)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-1.40017984  0.44200116]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P0RsPo1dV1gT"},"source":["### Dyna Q w/ epsilon = 0.7"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nDoAMRkV8aH","executionInfo":{"elapsed":486806,"status":"ok","timestamp":1607470897893,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"3c035792-4c15-4190-c1ed-0880473793f0"},"source":["# run dyna q-learning on first environment\n","# epsilon = 0.7\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.01, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"planning_steps\": 10, \"terminal_state\": 4, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = DynaQAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","dyna_q_0_7 = np.mean(np.array(average_weight), 0)\n","print(dyna_q_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(dyna_q_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(dyna_q_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.62082794  0.21750325]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nlSyextzYgpg"},"source":["### Dyna Q w/ epsilon = 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOzrwY9gYkic","executionInfo":{"elapsed":368294,"status":"ok","timestamp":1607471420735,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"3813da89-6ea9-407a-ab09-358c493b0ae8"},"source":["# run dyna q-learning on first environment\n","# epsilon = 0.7\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.1, \"target_epsilon\": 0.01, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"planning_steps\": 10, \"terminal_state\": 4, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = DynaQAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","dyna_q_0_1 = np.mean(np.array(average_weight), 0)\n","print(dyna_q_0_1)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(dyna_q_0_1)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(dyna_q_0_1)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.68827058  0.23525888]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dksKKxGEhqJC"},"source":["### Run q learning for 10x as long to compare to Dyna Q"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4-3wIRMfe7r","executionInfo":{"elapsed":444996,"status":"ok","timestamp":1607471980282,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"1b789be6-1b42-4d09-ef2a-30ff29e62881"},"source":["# run q-learning on first environment\n","# epsilon = 0.7\n","# average over 100 runs\n","# each run consists of 100,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.7, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 100000\n","\n","env = SmallGridworld()\n","\n","agent = QLAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","q_0_7 = np.mean(np.array(average_weight), 0)\n","print(q_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(q_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(q_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.29493802  0.29616166]\n","Action in state 1: 1\n","Action in state 4: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2vg8daR3ny99"},"source":["### Off-policy Expected Sarsa, Behaviour Epsilon 0.7"]},{"cell_type":"markdown","metadata":{"id":"YqOQpQjkoBJO"},"source":["#### Target Epsilon 0.6"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIbNKLDsoGqa","executionInfo":{"elapsed":96224,"status":"ok","timestamp":1607472360317,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"fc295c32-a613-4750-c2e4-3355a9b80f38"},"source":["# run expected sarsa on first environment (off policy)\n","# behaviour epsilon = 0.7\n","# target epsilon = 0.6\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.6, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.43268467  0.30985329]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bfbRHXWMphma"},"source":["#### Target Epsilon 0.5"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFswdDiAp3Ld","executionInfo":{"elapsed":96161,"status":"ok","timestamp":1607472467616,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"108d7b75-aec2-49a8-e36a-c724866d682f"},"source":["# run expected sarsa on first environment (off policy)\n","# behaviour epsilon = 0.7\n","# target epsilon = 0.5\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.5, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.38642381  0.30300923]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rmCt9Ylp1J-d"},"source":["#### Target Epsilon 0.4"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iu5b4fAk1W4C","executionInfo":{"elapsed":96525,"status":"ok","timestamp":1607472723612,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"fc0e09e3-c2a5-4022-b051-41f04564030c"},"source":["# run expected sarsa on first environment (off policy)\n","# behaviour epsilon = 0.7\n","# target epsilon = 0.4\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.4, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.40006164  0.30839644]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CECq2xwV1cBn"},"source":["#### Target Epsilon 0.3"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KySZkMm514Ln","executionInfo":{"elapsed":96997,"status":"ok","timestamp":1607472854662,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"5904cc3a-1225-40f9-bae2-3d7216169393"},"source":["# run expected sarsa on first environment (off policy)\n","# behaviour epsilon = 0.7\n","# target epsilon = 0.3\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.3, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.37179261  0.30433027]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m_iXzNiM19J_"},"source":["#### Target Epsilon 0.2"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EhjT6dn2Xj_","executionInfo":{"elapsed":97983,"status":"ok","timestamp":1607473263479,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"42ba7138-ae4f-49a6-fc9f-86130c0aacde"},"source":["# run expected sarsa on first environment (off policy)\n","# behaviour epsilon = 0.7\n","# target epsilon = 0.2\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.2, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.35022603  0.30334207]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iAmmhl9t23gK"},"source":["#### Target Epsilon 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZS4BXsV29XV","executionInfo":{"elapsed":97174,"status":"ok","timestamp":1607473396001,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"6cbf8579-5acf-4129-d057-ea238d5a1e9f"},"source":["# run expected sarsa on first environment (off policy)\n","# behaviour epsilon = 0.7\n","# target epsilon = 0.1\n","# average over 100 runs\n","# each run consists of 10,000 episodes\n","\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.1, \"gamma\": 1, \"alpha\": 0.05, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100\n","num_episodes = 10000\n","\n","env = SmallGridworld()\n","\n","agent = ExAgent()\n","\n","average_weight = []\n","\n","for run in range(num_runs):\n","    #initialize environment and agent\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    for episode in range(num_episodes):\n","        #start the episode with first state, action\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","        is_terminal = False\n","\n","        #while episode has not terminated agent takes steps\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            #if in terminal state end episode\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            #if epsiode hasn't ended then agent takes action\n","            action = agent.agent_step(reward, state)\n","\n","        #episode has ended, re-set env\n","        env.env_cleanup()\n","    \n","    #run has ended, add final weight vector to the list of learned weights\n","    average_weight.append(agent.w)\n","\n","#all runs have ended, get average of weights learned on all runs\n","e_sarsa_0_7 = np.mean(np.array(average_weight), 0)\n","print(e_sarsa_0_7)\n","#confirm actions the agent takes in states 1 and 4\n","action_1 = np.argmax(np.dot(agent.features[0], np.array(e_sarsa_0_7)))\n","print(\"Action in state 1: {}\".format(action_1 + 1))\n","action_4 = np.argmax(np.dot(agent.features[3], np.array(e_sarsa_0_7)))\n","print(\"Action in state 4: {}\".format(action_4 + 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-0.31102284  0.29943442]\n","Action in state 1: 1\n","Action in state 4: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ngz2E-Rfpf-m"},"source":["## Second set of experiments"]},{"cell_type":"markdown","metadata":{"id":"VjuMOItQvgr5"},"source":["###  epsilon = 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bwVtRXHIpnlB","executionInfo":{"elapsed":5701941,"status":"ok","timestamp":1607633331261,"user":{"displayName":"Eric Austin","photoUrl":"","userId":"15276873954253857195"},"user_tz":420},"outputId":"0238f68a-c6bb-432f-eb24-6318824e4a7e"},"source":["from tqdm import tqdm\n","\n","num_runs = 100  # The number of runs\n","num_episodes = 1000  # The number of episodes in each run\n","length_episode = 60\n","max_alpha = 0.001\n","min_alpha = 0.0001\n","\n","env_info = {\"grid_height\": 6, \"grid_width\": 6}\n","agent_info = {\"epsilon\": 0.1, \"target_epsilon\": 0.1, \"gamma\": 0.95, \"alpha\": max_alpha, \"initial_weights\": 0.0,\n","              \"num_states\": 6 * 6, \"num_actions\": 4,\n","              \"dimensions\": 140, \"is_testing\": False,\n","              \"planning_steps\": 10, \"terminal_state\": -1, \"features\": np.random.randn(6 * 6, 4, 140)}\n","\n","\n","def training(env, agent):\n","    agent.is_testing = False\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","def testing(env, agent):\n","    agent.is_testing = True\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","env = GridWorldEnvironment()\n","# agent = QLAgent()\n","\n","\n","agent_dic = {\"sarsa\": SarsaAgent(), \"q_learning\": QLAgent(), \"expected_sarsa\": ExAgent(),\n","             \"double_q_learning\": DoubleQLAgent(),\n","             \"dyna_q\": DynaQAgent()}\n","# \"q_learning_tabular\": QLNFAgent()\n","\n","\n","for agent_name in agent_dic:\n","    print(agent_name)\n","    run_reward = []\n","    for run in tqdm(range(num_runs)):\n","\n","        env.env_init(env_info)\n","        agent_dic[agent_name].agent_init(agent_info)\n","\n","\n","        episode_rewards = []\n","\n","        for episode in range(num_episodes):\n","            training_reward = training(env, agent_dic[agent_name])\n","            testing_reward = testing(env, agent_dic[agent_name])\n","\n","            agent_dic[agent_name].alpha -= (max_alpha - min_alpha) / num_episodes\n","\n","            episode_rewards.append(testing_reward)\n","\n","        run_reward.append(episode_rewards)\n","\n","    run_reward = np.mean(np.array(run_reward), 0)\n","\n","    np.save(agent_name + \".npy\", run_reward)\n","\n","    string_name = agent_name.replace(\"_\", \" \")\n","    plt.plot(run_reward, label=string_name)\n","plt.xlabel(\"Episodes\")\n","plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\", rotation=0, labelpad=40)\n","plt.xlim(0, num_episodes)\n","plt.ylim(0, 600)\n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["sarsa\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:02<04:32,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:05<04:33,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:08<04:31,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:11<04:27,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:14<04:25,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [00:16<04:21,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [00:19<04:21,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [00:22<04:18,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [00:25<04:15,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [00:28<04:13,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [00:30<04:09,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [00:33<04:08,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [00:36<04:05,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [00:39<04:03,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [00:42<03:58,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [00:44<03:54,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [00:47<03:52,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [00:50<03:49,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [00:53<03:46,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [00:56<03:42,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [00:58<03:40,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [01:01<03:37,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [01:04<03:34,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [01:07<03:31,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [01:10<03:29,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [01:12<03:26,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [01:15<03:24,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [01:18<03:22,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [01:21<03:20,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [01:24<03:17,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [01:26<03:13,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [01:29<03:10,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [01:32<03:08,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [01:35<03:05,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [01:38<03:02,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [01:40<03:00,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [01:43<02:59,  2.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [01:47<03:05,  2.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [01:50<03:04,  3.03s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [01:53<02:58,  2.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [01:56<02:52,  2.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [01:58<02:47,  2.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [02:01<02:43,  2.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [02:04<02:39,  2.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [02:07<02:35,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [02:09<02:31,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [02:12<02:28,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [02:15<02:26,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [02:18<02:24,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [02:21<02:22,  2.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [02:24<02:19,  2.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [02:26<02:15,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [02:29<02:12,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [02:32<02:08,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [02:35<02:04,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [02:37<02:01,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [02:40<01:58,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [02:43<01:55,  2.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [02:46<01:52,  2.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [02:48<01:50,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [02:51<01:46,  2.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [02:54<01:44,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [02:57<01:41,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [03:00<01:39,  2.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [03:02<01:37,  2.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [03:05<01:33,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [03:08<01:30,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [03:10<01:27,  2.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [03:13<01:25,  2.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [03:16<01:22,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [03:19<01:21,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [03:22<01:17,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [03:24<01:15,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [03:27<01:12,  2.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [03:30<01:09,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [03:33<01:07,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [03:36<01:04,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [03:38<01:01,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [03:41<00:58,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [03:44<00:55,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [03:47<00:53,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [03:50<00:50,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [03:52<00:47,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [03:55<00:44,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [03:58<00:42,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [04:01<00:39,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [04:03<00:36,  2.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [04:06<00:33,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [04:09<00:30,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [04:12<00:27,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [04:15<00:25,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [04:17<00:22,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [04:20<00:19,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [04:23<00:16,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [04:26<00:13,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [04:29<00:11,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [04:31<00:08,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [04:34<00:05,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [04:37<00:02,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [04:40<00:00,  2.80s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["q_learning\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:04<07:11,  4.36s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:08<07:09,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:13<07:03,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:17<06:59,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:21<06:56,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [00:26<06:50,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [00:30<06:44,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [00:34<06:41,  4.36s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [00:39<06:34,  4.34s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [00:43<06:31,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [00:47<06:24,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [00:52<06:22,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [00:56<06:19,  4.36s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [01:01<06:15,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [01:05<06:11,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [01:09<06:08,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [01:14<06:03,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [01:18<05:57,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [01:22<05:55,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [01:27<05:48,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [01:31<05:41,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [01:35<05:36,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [01:40<05:30,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [01:44<05:26,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [01:48<05:23,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [01:53<05:20,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [01:57<05:13,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [02:01<05:11,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [02:05<05:06,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [02:10<05:02,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [02:14<05:01,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [02:19<04:59,  4.41s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [02:23<04:51,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [02:27<04:44,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [02:32<04:40,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [02:36<04:37,  4.34s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [02:40<04:33,  4.34s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [02:45<04:27,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [02:49<04:25,  4.34s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [02:53<04:23,  4.39s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [02:58<04:20,  4.42s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [03:02<04:18,  4.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [03:07<04:15,  4.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [03:11<04:08,  4.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [03:16<04:00,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [03:20<03:56,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [03:24<03:49,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [03:29<03:45,  4.34s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [03:33<03:39,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [03:37<03:34,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [03:41<03:31,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [03:46<03:27,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [03:50<03:21,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [03:54<03:18,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [03:59<03:14,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [04:03<03:09,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [04:07<03:05,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [04:12<03:01,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [04:16<02:57,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [04:20<02:52,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [04:25<02:48,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [04:29<02:44,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [04:33<02:39,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [04:37<02:34,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [04:42<02:29,  4.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [04:46<02:26,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [04:50<02:22,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [04:55<02:18,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [04:59<02:13,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [05:03<02:08,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [05:08<02:04,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [05:12<02:00,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [05:16<01:55,  4.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [05:20<01:52,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [05:25<01:47,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [05:29<01:43,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [05:33<01:39,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [05:38<01:34,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [05:42<01:30,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [05:46<01:26,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [05:51<01:22,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [05:55<01:18,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [06:00<01:13,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [06:04<01:09,  4.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [06:08<01:04,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [06:12<01:00,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [06:17<00:55,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [06:21<00:51,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [06:25<00:47,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [06:30<00:42,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [06:34<00:38,  4.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [06:38<00:34,  4.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [06:42<00:30,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [06:47<00:25,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [06:51<00:21,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [06:56<00:17,  4.36s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [07:00<00:13,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [07:04<00:08,  4.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [07:09<00:04,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [07:13<00:00,  4.34s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["expected_sarsa\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:10<16:44, 10.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:20<16:31, 10.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:30<16:14, 10.05s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:40<16:02, 10.03s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:50<15:53, 10.03s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [01:00<15:46, 10.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [01:10<15:48, 10.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [01:20<15:38, 10.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [01:31<15:24, 10.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [01:40<15:06, 10.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [01:51<14:57, 10.08s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [02:00<14:43, 10.04s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [02:11<14:36, 10.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [02:21<14:24, 10.05s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [02:31<14:11, 10.01s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [02:41<14:02, 10.04s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [02:50<13:48,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [03:01<13:41, 10.01s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [03:10<13:28,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [03:20<13:18,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [03:30<13:04,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [03:40<12:53,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [03:50<12:46,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [04:00<12:34,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [04:10<12:23,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [04:20<12:15,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [04:40<11:54,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [04:50<11:41,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [04:59<11:28,  9.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [05:09<11:18,  9.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [05:20<11:19,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [05:29<11:06,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [05:39<10:57,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [05:49<10:48,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [05:59<10:35,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [06:09<10:27,  9.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [06:19<10:19, 10.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [06:29<10:12, 10.04s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [06:39<10:01, 10.03s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [06:49<09:50, 10.01s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [06:59<09:39, 10.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [07:09<09:28,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [07:19<09:19,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [07:29<09:09,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [07:39<08:59, 10.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [07:49<08:47,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [07:59<08:36,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [08:09<08:28,  9.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [08:19<08:19,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [08:29<08:07,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [08:39<07:55,  9.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [08:49<07:46,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [08:59<07:35,  9.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [09:09<07:28,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [09:19<07:19, 10.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [09:29<07:07,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [09:39<06:58,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [09:49<06:48,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [09:58<06:36,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [10:08<06:27,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [10:18<06:16,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [10:28<06:08,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [10:38<05:58,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [10:48<05:47,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [10:58<05:36,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [11:08<05:25,  9.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [11:18<05:15,  9.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [11:27<05:05,  9.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [11:37<04:57,  9.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [11:48<04:49,  9.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [11:57<04:38,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [12:08<04:29,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [12:17<04:18,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [12:27<04:07,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [12:37<03:57,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [12:47<03:46,  9.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [12:57<03:38,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [13:07<03:27,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [13:17<03:19,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [13:27<03:08,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [13:37<02:59,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [13:47<02:49,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [13:57<02:38,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [14:06<02:28,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [14:17<02:20, 10.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [14:27<02:11, 10.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [14:37<02:01, 10.09s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [14:47<01:50, 10.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [14:57<01:40, 10.01s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [15:07<01:29,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [15:17<01:19,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [15:27<01:09,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [15:37<01:00, 10.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [15:47<00:50, 10.05s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [15:57<00:40, 10.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [16:07<00:30, 10.03s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [16:17<00:20, 10.04s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [16:27<00:10, 10.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [16:37<00:00,  9.98s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["double_q_learning\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:08<14:15,  8.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:17<14:19,  8.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:26<14:18,  8.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:34<13:49,  8.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:43<13:31,  8.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [00:51<13:13,  8.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [00:59<12:57,  8.36s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [01:07<12:48,  8.36s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [01:16<12:34,  8.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [01:24<12:23,  8.26s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [01:32<12:11,  8.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [01:40<12:02,  8.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [01:48<11:54,  8.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [01:57<11:46,  8.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [02:05<11:36,  8.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [02:13<11:25,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [02:21<11:14,  8.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [02:29<11:08,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [02:37<10:59,  8.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [02:45<10:52,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [02:54<10:45,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [03:02<10:34,  8.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [03:10<10:29,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [03:18<10:18,  8.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [03:26<10:11,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [03:34<09:59,  8.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [03:42<09:50,  8.08s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [03:50<09:44,  8.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [03:58<09:36,  8.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [04:07<09:37,  8.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [04:15<09:26,  8.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [04:23<09:16,  8.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [04:31<09:08,  8.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [04:40<09:01,  8.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [04:48<08:54,  8.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [04:56<08:44,  8.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [05:04<08:34,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [05:12<08:23,  8.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [05:20<08:14,  8.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [05:28<08:06,  8.10s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [05:37<08:02,  8.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [05:45<08:00,  8.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [05:54<07:52,  8.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [06:02<07:41,  8.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [06:10<07:33,  8.24s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [06:18<07:23,  8.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [06:26<07:12,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [06:34<07:04,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [06:42<06:56,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [06:51<06:48,  8.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [06:59<06:40,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [07:07<06:32,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [07:15<06:23,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [07:23<06:14,  8.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [07:31<06:05,  8.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [07:39<05:57,  8.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [07:47<05:47,  8.08s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [07:56<05:42,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [08:04<05:34,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [08:12<05:28,  8.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [08:20<05:21,  8.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [08:29<05:11,  8.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [08:37<05:03,  8.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [08:45<04:55,  8.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [08:53<04:46,  8.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [09:01<04:38,  8.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [09:10<04:31,  8.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [09:18<04:23,  8.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [09:26<04:14,  8.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [09:34<04:06,  8.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [09:42<03:57,  8.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [09:51<03:48,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [09:59<03:39,  8.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [10:07<03:30,  8.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [10:15<03:22,  8.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [10:23<03:15,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [10:31<03:06,  8.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [10:39<02:58,  8.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [10:47<02:50,  8.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [10:56<02:43,  8.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [11:04<02:35,  8.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [11:12<02:27,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [11:20<02:18,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [11:28<02:10,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [11:36<02:02,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [11:45<01:54,  8.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [11:53<01:46,  8.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [12:01<01:38,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [12:09<01:30,  8.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [12:17<01:21,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [12:25<01:13,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [12:34<01:05,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [12:42<00:56,  8.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [12:50<00:48,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [12:58<00:40,  8.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [13:06<00:32,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [13:14<00:24,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [13:23<00:16,  8.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [13:31<00:08,  8.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [13:39<00:00,  8.19s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["dyna_q\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:31<52:13, 31.65s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [01:03<51:59, 31.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [01:35<51:29, 31.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [02:07<50:57, 31.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [02:40<50:45, 32.05s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [03:12<50:08, 32.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [03:44<49:38, 32.03s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [04:16<49:03, 32.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [04:47<48:23, 31.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [05:19<47:47, 31.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [05:51<47:17, 31.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [06:23<46:44, 31.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [06:54<45:54, 31.66s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [07:26<45:25, 31.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [07:59<45:24, 32.05s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [08:30<44:41, 31.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [09:02<44:04, 31.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [09:34<43:31, 31.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [10:05<42:51, 31.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [10:36<41:44, 31.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [11:07<41:23, 31.43s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [11:39<40:48, 31.39s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [12:10<40:21, 31.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [12:42<39:53, 31.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [13:14<39:44, 31.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [13:46<39:12, 31.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [14:18<38:40, 31.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [14:50<38:08, 31.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [15:22<37:45, 31.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [15:54<37:16, 31.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [16:26<36:48, 32.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [16:58<36:10, 31.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [17:29<35:20, 31.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [18:00<34:48, 31.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [18:32<34:11, 31.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [19:03<33:38, 31.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [19:35<33:03, 31.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [20:06<32:35, 31.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [20:38<32:05, 31.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [21:09<31:28, 31.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [21:41<30:59, 31.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [22:13<30:44, 31.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [22:45<30:15, 31.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [23:17<29:39, 31.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [23:49<29:14, 31.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [24:21<28:39, 31.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [24:52<28:00, 31.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [25:24<27:30, 31.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [25:55<26:54, 31.66s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [26:27<26:21, 31.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [26:58<25:48, 31.61s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [27:30<25:15, 31.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [28:01<24:40, 31.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [28:32<23:59, 31.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [29:04<23:35, 31.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [29:36<23:06, 31.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [30:07<22:33, 31.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [30:39<22:04, 31.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [31:10<21:34, 31.56s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [31:42<21:05, 31.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [32:13<20:30, 31.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [32:45<19:58, 31.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [33:17<19:27, 31.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [33:49<19:02, 31.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [34:21<18:38, 31.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [34:53<18:07, 32.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [35:25<17:32, 31.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [35:57<16:58, 31.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [36:28<16:24, 31.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [37:00<15:54, 31.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [37:32<15:21, 31.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [38:02<14:40, 31.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [38:33<14:02, 31.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [39:05<13:34, 31.33s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [39:35<12:58, 31.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [40:07<12:31, 31.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [40:39<12:01, 31.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [41:10<11:32, 31.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [41:43<11:05, 31.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [42:15<10:35, 31.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [42:46<10:03, 31.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [43:18<09:31, 31.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [43:50<08:59, 31.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [44:21<08:27, 31.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [44:53<07:56, 31.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [45:25<07:25, 31.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [45:57<06:53, 31.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [46:29<06:20, 31.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [47:00<05:49, 31.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [47:32<05:17, 31.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [48:04<04:45, 31.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [48:35<04:13, 31.65s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [49:07<03:41, 31.65s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [49:38<03:09, 31.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [50:10<02:38, 31.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [50:42<02:06, 31.67s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [51:13<01:34, 31.56s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [51:45<01:03, 31.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [52:18<00:31, 31.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [52:49<00:00, 31.70s/it]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"hqYWuzDRu7tE"},"source":["### epsilon = 0.7"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ItJZHL4-u_sK","outputId":"199f30fb-2196-49aa-e56b-3234fb2b919d"},"source":["from tqdm import tqdm\n","\n","num_runs = 100  # The number of runs\n","num_episodes = 1000  # The number of episodes in each run\n","length_episode = 60\n","max_alpha = 0.001\n","min_alpha = 0.0001\n","\n","env_info = {\"grid_height\": 6, \"grid_width\": 6}\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.7, \"gamma\": 0.95, \"alpha\": max_alpha, \"initial_weights\": 0.0,\n","              \"num_states\": 6 * 6, \"num_actions\": 4,\n","              \"dimensions\": 140, \"is_testing\": False,\n","              \"planning_steps\": 10, \"terminal_state\": -1, \"features\": np.random.randn(6 * 6, 4, 140)}\n","\n","\n","def training(env, agent):\n","    agent.is_testing = False\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","def testing(env, agent):\n","    agent.is_testing = True\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","env = GridWorldEnvironment()\n","# agent = QLAgent()\n","\n","\n","agent_dic = {\"sarsa\": SarsaAgent(), \"q_learning\": QLAgent(), \"expected_sarsa\": ExAgent(),\n","             \"double_q_learning\": DoubleQLAgent(),\n","             \"dyna_q\": DynaQAgent()}\n","# \"q_learning_tabular\": QLNFAgent()\n","\n","\n","for agent_name in agent_dic:\n","    print(agent_name)\n","    run_reward = []\n","    for run in tqdm(range(num_runs)):\n","\n","        env.env_init(env_info)\n","        agent_dic[agent_name].agent_init(agent_info)\n","\n","\n","        episode_rewards = []\n","\n","        for episode in range(num_episodes):\n","            training_reward = training(env, agent_dic[agent_name])\n","            testing_reward = testing(env, agent_dic[agent_name])\n","\n","            agent_dic[agent_name].alpha -= (max_alpha - min_alpha) / num_episodes\n","\n","            episode_rewards.append(testing_reward)\n","\n","\n","        run_reward.append(episode_rewards)\n","\n","    run_reward = np.mean(np.array(run_reward), 0)\n","\n","    np.save(agent_name + \".npy\", run_reward)\n","\n","    string_name = agent_name.replace(\"_\", \" \")\n","    plt.plot(run_reward, label=string_name)\n","plt.xlabel(\"Episodes\")\n","plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\", rotation=0, labelpad=40)\n","plt.xlim(0, num_episodes)\n","plt.ylim(0, 200)\n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["sarsa\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:02<04:26,  2.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:05<04:23,  2.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:08<04:19,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:11<04:26,  2.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:13<04:28,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [00:16<04:27,  2.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [00:19<04:23,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [00:22<04:21,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [00:25<04:21,  2.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [00:28<04:23,  2.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [00:31<04:21,  2.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [00:34<04:10,  2.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [00:36<04:03,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [00:39<03:59,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [00:42<03:53,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [00:44<03:49,  2.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [00:47<03:45,  2.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [00:50<03:41,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [00:52<03:38,  2.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [00:55<03:34,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [00:58<03:31,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [01:00<03:28,  2.67s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [01:03<03:25,  2.67s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [01:06<03:28,  2.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [01:09<03:28,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [01:12<03:27,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [01:15<03:27,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [01:17<03:24,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [01:20<03:22,  2.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [01:23<03:18,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [01:26<03:15,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [01:29<03:11,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [01:32<03:09,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [01:34<03:05,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [01:37<03:03,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [01:40<03:01,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [01:43<02:58,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [01:46<02:55,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [01:49<02:52,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [01:52<02:52,  2.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [01:55<03:04,  3.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [01:59<03:03,  3.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [02:01<02:55,  3.08s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [02:04<02:46,  2.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [02:07<02:40,  2.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [02:10<02:34,  2.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [02:12<02:30,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [02:15<02:25,  2.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [02:18<02:22,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [02:21<02:18,  2.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [02:23<02:14,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [02:26<02:12,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [02:29<02:10,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [02:32<02:07,  2.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [02:35<02:06,  2.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [02:37<02:04,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [02:40<02:01,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [02:43<01:58,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [02:46<01:56,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [02:49<01:53,  2.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [02:52<01:50,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [02:55<01:48,  2.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [02:57<01:45,  2.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [03:00<01:43,  2.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [03:03<01:39,  2.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [03:06<01:37,  2.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [03:09<01:34,  2.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [03:12<01:30,  2.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [03:14<01:27,  2.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [03:17<01:23,  2.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [03:20<01:20,  2.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [03:23<01:16,  2.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [03:25<01:14,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [03:28<01:11,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [03:31<01:08,  2.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [03:33<01:05,  2.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [03:36<01:02,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [03:39<00:59,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [03:41<00:56,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [03:44<00:53,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [03:47<00:51,  2.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [03:50<00:48,  2.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [03:52<00:46,  2.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [03:55<00:43,  2.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [03:58<00:41,  2.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [04:01<00:38,  2.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [04:03<00:35,  2.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [04:06<00:32,  2.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [04:09<00:29,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [04:11<00:26,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [04:14<00:24,  2.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [04:17<00:21,  2.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [04:19<00:18,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [04:22<00:16,  2.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [04:25<00:13,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [04:27<00:10,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [04:30<00:08,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [04:33<00:05,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [04:35<00:02,  2.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [04:38<00:00,  2.79s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["q_learning\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:04<06:58,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:08<07:00,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:12<06:56,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:17<06:50,  4.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:21<06:42,  4.24s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [00:25<06:35,  4.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [00:29<06:33,  4.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [00:34<06:30,  4.24s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [00:38<06:25,  4.24s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [00:42<06:19,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [00:46<06:14,  4.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [00:50<06:11,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [00:55<06:13,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [01:00<06:27,  4.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [01:05<06:27,  4.56s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [01:09<06:13,  4.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [01:13<06:06,  4.41s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [01:17<05:56,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [01:21<05:48,  4.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [01:26<05:41,  4.27s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [01:30<05:35,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [01:34<05:29,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [01:38<05:24,  4.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [01:42<05:18,  4.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [01:46<05:12,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [01:51<05:08,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [01:55<05:04,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [01:59<05:04,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [02:03<05:02,  4.27s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [02:08<04:58,  4.26s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [02:12<04:53,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [02:16<04:47,  4.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [02:20<04:45,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [02:25<04:49,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [02:30<04:44,  4.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [02:34<04:36,  4.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [02:38<04:30,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [02:42<04:24,  4.27s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [02:46<04:19,  4.26s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [02:51<04:14,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [02:55<04:10,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [02:59<04:06,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [03:03<04:01,  4.24s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [03:08<03:56,  4.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [03:12<03:52,  4.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [03:16<03:47,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [03:20<03:43,  4.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [03:24<03:38,  4.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [03:28<03:33,  4.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [03:33<03:28,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [03:37<03:24,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [03:41<03:19,  4.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [03:45<03:16,  4.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [03:49<03:12,  4.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [03:54<03:07,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [03:58<03:04,  4.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [04:02<03:00,  4.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [04:06<02:58,  4.24s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [04:10<02:53,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [04:15<02:50,  4.25s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [04:19<02:46,  4.26s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [04:23<02:42,  4.26s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [04:28<02:36,  4.23s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [04:32<02:31,  4.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [04:36<02:26,  4.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [04:40<02:22,  4.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [04:44<02:18,  4.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [04:48<02:14,  4.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [04:53<02:09,  4.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [04:57<02:05,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [05:01<02:01,  4.18s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [05:05<01:56,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [05:09<01:52,  4.17s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [05:14<01:49,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [05:18<01:45,  4.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [05:22<01:41,  4.24s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [05:26<01:36,  4.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [05:31<01:34,  4.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [05:35<01:31,  4.35s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [05:40<01:29,  4.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [05:44<01:24,  4.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [05:49<01:21,  4.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [05:53<01:16,  4.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [05:58<01:12,  4.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [06:03<01:07,  4.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [06:07<01:03,  4.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [06:12<00:59,  4.59s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [06:17<00:55,  4.62s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [06:21<00:49,  4.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [06:25<00:44,  4.43s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [06:29<00:39,  4.39s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [06:34<00:34,  4.34s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [06:38<00:30,  4.31s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [06:42<00:25,  4.27s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [06:46<00:21,  4.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [06:51<00:17,  4.27s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [06:55<00:12,  4.26s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [06:59<00:08,  4.27s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [07:03<00:04,  4.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [07:08<00:00,  4.28s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["expected_sarsa\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:10<17:19, 10.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:21<17:12, 10.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:33<17:49, 11.02s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:43<17:25, 10.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:54<17:07, 10.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [01:05<16:58, 10.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [01:15<16:40, 10.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [01:26<16:23, 10.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [01:37<16:11, 10.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [01:47<15:54, 10.61s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [01:57<15:29, 10.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [02:07<15:08, 10.32s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [02:17<14:48, 10.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [02:27<14:29, 10.11s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [02:37<14:15, 10.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [02:47<14:04, 10.05s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [02:57<13:52, 10.04s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [03:07<13:42, 10.03s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [03:17<13:28,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [03:27<13:15,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [03:37<13:04,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [03:47<12:54,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [03:56<12:43,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [04:06<12:35,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [04:17<12:46, 10.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [04:28<12:47, 10.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [04:38<12:30, 10.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [04:48<12:13, 10.19s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [04:58<12:00, 10.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [05:08<11:48, 10.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [05:18<11:36, 10.09s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [05:28<11:24, 10.06s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [05:38<11:15, 10.08s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [05:48<11:01, 10.02s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [05:58<10:49,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [06:08<10:38,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [06:18<10:27,  9.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [06:28<10:16,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [06:38<10:06,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [06:48<09:55,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [06:58<09:45,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [07:08<09:34,  9.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [07:17<09:24,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [07:27<09:14,  9.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [07:37<09:04,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [07:47<08:54,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [07:57<08:44,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [08:07<08:33,  9.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [08:17<08:23,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [08:27<08:13,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [08:36<08:04,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [08:46<07:54,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [08:56<07:44,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [09:06<07:35,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [09:16<07:25,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [09:26<07:15,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [09:36<07:10, 10.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [09:47<07:15, 10.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [09:57<07:00, 10.26s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [10:07<06:45, 10.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [10:17<06:31, 10.05s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [10:27<06:19,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [10:37<06:08,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [10:47<05:59,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [10:57<05:48,  9.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [11:07<05:37,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [11:17<05:27,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [11:26<05:16,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [11:36<05:06,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [11:46<04:56,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [11:56<04:46,  9.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [12:06<04:36,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [12:16<04:26,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [12:26<04:17,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [12:36<04:07,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [12:45<03:57,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [12:55<03:48,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [13:06<03:39,  9.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [13:16<03:29,  9.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [13:25<03:19,  9.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [13:35<03:08,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [13:45<02:58,  9.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [13:55<02:48,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [14:05<02:38,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [14:15<02:28,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [14:25<02:18,  9.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [14:35<02:08,  9.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [14:44<01:58,  9.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [14:54<01:48,  9.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [15:04<01:39,  9.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [15:14<01:29,  9.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [15:24<01:19,  9.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [15:34<01:09,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [15:44<00:59,  9.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [15:54<00:49,  9.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [16:04<00:39,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [16:14<00:29,  9.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [16:23<00:19,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [16:33<00:09,  9.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [16:43<00:00, 10.04s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["double_q_learning\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:07<13:04,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [00:15<12:54,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [00:23<12:45,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [00:31<12:36,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [00:39<12:28,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [00:47<12:20,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [00:55<12:12,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [01:03<12:05,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [01:10<11:57,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [01:18<11:48,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [01:26<11:41,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [01:34<11:34,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [01:42<11:25,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [01:50<11:18,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [01:58<11:10,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [02:06<11:03,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [02:13<10:53,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [02:21<10:45,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [02:29<10:37,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [02:37<10:30,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [02:45<10:23,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [02:53<10:15,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [03:01<10:07,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [03:09<10:00,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [03:17<09:52,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [03:24<09:43,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [03:33<09:39,  7.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [03:40<09:32,  7.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [03:48<09:23,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [03:56<09:13,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [04:04<09:07,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [04:12<09:02,  7.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [04:20<08:53,  7.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [04:28<08:45,  7.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [04:36<08:35,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [04:44<08:26,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [04:52<08:18,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [05:00<08:09,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [05:08<08:01,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [05:15<07:53,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [05:23<07:46,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [05:31<07:38,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [05:39<07:29,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [05:47<07:23,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [05:55<07:16,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [06:03<07:08,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [06:11<07:00,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [06:19<06:51,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [06:27<06:43,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [06:35<06:36,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [06:43<06:28,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [06:51<06:20,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [06:58<06:11,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [07:06<06:03,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [07:14<05:55,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [07:22<05:47,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [07:30<05:39,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [07:38<05:31,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [07:46<05:22,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [07:54<05:14,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [08:01<05:06,  7.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [08:09<04:59,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [08:17<04:51,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [08:25<04:43,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [08:33<04:35,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [08:41<04:29,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [08:49<04:21,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [08:57<04:15,  7.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [09:05<04:06,  7.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [09:13<03:58,  7.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [09:21<03:51,  7.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [09:29<03:42,  7.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [09:37<03:34,  7.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [09:45<03:25,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [09:52<03:17,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [10:00<03:09,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [10:08<03:01,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [10:16<02:53,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [10:24<02:45,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [10:32<02:37,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [10:40<02:29,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [10:48<02:21,  7.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [10:56<02:13,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [11:03<02:06,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [11:11<01:58,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [11:19<01:50,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [11:27<01:42,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [11:35<01:34,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [11:43<01:26,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [11:51<01:18,  7.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [11:59<01:11,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [12:07<01:03,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [12:15<00:55,  7.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [12:22<00:47,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [12:30<00:39,  7.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [12:38<00:31,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [12:46<00:23,  7.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [12:54<00:15,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [13:02<00:07,  7.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [13:10<00:00,  7.90s/it]\n","\n","\n","\n","  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["dyna_q\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","  1%|          | 1/100 [00:29<49:26, 29.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 2/100 [01:00<49:02, 30.02s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  3%|▎         | 3/100 [01:30<48:36, 30.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 4/100 [01:59<47:54, 29.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  5%|▌         | 5/100 [02:29<47:16, 29.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 6/100 [02:59<46:42, 29.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  7%|▋         | 7/100 [03:29<46:12, 29.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 8/100 [03:58<45:40, 29.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  9%|▉         | 9/100 [04:28<45:07, 29.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|█         | 10/100 [04:58<44:38, 29.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 11%|█         | 11/100 [05:27<44:05, 29.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 12/100 [05:57<43:31, 29.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 13/100 [06:27<43:20, 29.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 14%|█▍        | 14/100 [06:57<42:50, 29.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 15/100 [07:27<42:14, 29.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 16%|█▌        | 16/100 [07:57<41:39, 29.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 17/100 [08:26<41:08, 29.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 18%|█▊        | 18/100 [08:56<40:37, 29.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 19/100 [09:26<40:05, 29.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 20%|██        | 20/100 [09:55<39:37, 29.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 21/100 [10:25<39:08, 29.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 22%|██▏       | 22/100 [10:55<38:38, 29.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 23/100 [11:25<38:09, 29.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 24%|██▍       | 24/100 [11:56<38:11, 30.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 25/100 [12:25<37:31, 30.01s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 26%|██▌       | 26/100 [12:55<36:56, 29.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 27/100 [13:25<36:18, 29.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 28%|██▊       | 28/100 [13:54<35:45, 29.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 29/100 [14:24<35:15, 29.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 30%|███       | 30/100 [14:54<34:40, 29.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 31/100 [15:24<34:11, 29.74s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 32%|███▏      | 32/100 [15:53<33:40, 29.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 33/100 [16:23<33:11, 29.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 34%|███▍      | 34/100 [16:54<33:00, 30.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▌      | 35/100 [17:23<32:23, 29.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 36%|███▌      | 36/100 [17:53<31:46, 29.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 37/100 [18:23<31:15, 29.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 38/100 [18:52<30:47, 29.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 39%|███▉      | 39/100 [19:22<30:15, 29.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 40/100 [19:52<29:43, 29.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 41%|████      | 41/100 [20:21<29:12, 29.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 42/100 [20:51<28:43, 29.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 43%|████▎     | 43/100 [21:21<28:14, 29.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 44/100 [21:51<27:46, 29.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 45%|████▌     | 45/100 [22:21<27:19, 29.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 46/100 [22:50<26:47, 29.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 47%|████▋     | 47/100 [23:20<26:22, 29.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 48/100 [23:50<25:50, 29.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 49%|████▉     | 49/100 [24:20<25:19, 29.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 50/100 [24:50<24:49, 29.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 51%|█████     | 51/100 [25:19<24:18, 29.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 52/100 [25:49<23:44, 29.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 53%|█████▎    | 53/100 [26:18<23:14, 29.67s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 54/100 [26:48<22:44, 29.67s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 55%|█████▌    | 55/100 [27:18<22:16, 29.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 56/100 [27:48<21:50, 29.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 57%|█████▋    | 57/100 [28:18<21:20, 29.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 58/100 [28:48<20:53, 29.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 59%|█████▉    | 59/100 [29:18<20:25, 29.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|██████    | 60/100 [29:47<19:53, 29.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 61%|██████    | 61/100 [30:17<19:21, 29.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 62/100 [30:47<18:51, 29.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 63/100 [31:17<18:21, 29.77s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 64%|██████▍   | 64/100 [31:46<17:52, 29.80s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 65/100 [32:16<17:23, 29.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 66%|██████▌   | 66/100 [32:46<16:54, 29.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 67/100 [33:16<16:24, 29.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 68%|██████▊   | 68/100 [33:46<15:53, 29.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 69/100 [34:16<15:26, 29.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 70%|███████   | 70/100 [34:46<14:57, 29.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 71/100 [35:15<14:25, 29.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 72%|███████▏  | 72/100 [35:45<13:57, 29.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 73/100 [36:15<13:26, 29.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 74%|███████▍  | 74/100 [36:45<12:56, 29.86s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 75/100 [37:15<12:28, 29.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 76%|███████▌  | 76/100 [37:45<11:57, 29.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 77/100 [38:16<11:34, 30.20s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 78%|███████▊  | 78/100 [38:46<11:02, 30.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 79/100 [39:16<10:30, 30.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 80%|████████  | 80/100 [39:45<09:58, 29.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 81/100 [40:15<09:28, 29.90s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 82%|████████▏ | 82/100 [40:45<08:57, 29.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 83/100 [41:15<08:27, 29.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 84%|████████▍ | 84/100 [41:44<07:56, 29.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▌ | 85/100 [42:14<07:26, 29.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 86%|████████▌ | 86/100 [42:44<06:57, 29.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 87/100 [43:14<06:28, 29.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 88/100 [43:44<06:00, 30.02s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 89%|████████▉ | 89/100 [44:14<05:29, 29.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 90/100 [44:44<04:58, 29.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 91%|█████████ | 91/100 [45:14<04:29, 29.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 92/100 [45:44<03:58, 29.87s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 93%|█████████▎| 93/100 [46:14<03:29, 29.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 94/100 [46:44<02:59, 29.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 95%|█████████▌| 95/100 [47:13<02:29, 29.88s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 96/100 [47:44<01:59, 29.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 97%|█████████▋| 97/100 [48:13<01:29, 29.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 98/100 [48:43<00:59, 29.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 99%|█████████▉| 99/100 [49:13<00:29, 29.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 100/100 [49:43<00:00, 29.83s/it]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"V58vJbxLvS4r"},"source":["### change alpha"]},{"cell_type":"code","metadata":{"id":"zDP-04luvp5-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ooM-uTAK_YM"},"source":["## run 1x5 grid world"]},{"cell_type":"code","metadata":{"id":"ICZ4dGtpHmIW"},"source":["from tqdm import tqdm\n","\n","# all_reward_sums = {}  # Contains sum of rewards during episode\n","# all_state_visits = {}  # Contains state visit counts during the last 10 episodes\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","agent_info = {\"epsilon\": 0.5, \"target_epsilon\": 0.1, \"gamma\": 1, \"alpha\": 0.01, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","agent_info2 = {\"epsilon\": 0.5, \"target_epsilon\": 0.01, \"gamma\": 1, \"alpha\": 0.01, \"initial_weights\": 0.0,\n","               \"num_states\": 4, \"num_actions\": 2,\n","               \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100  # The number of runs\n","num_episodes = 50000  # The number of episodes in each run\n","\n","env = SmallGridworld()\n","\n","agent = SarsaAgent()\n","agent2 = QLAgent()\n","\n","average_weight1 = []\n","average_weight2 = []\n","\n","# for alpha in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]:\n","for alpha in [0.1]:\n","\n","    agent_info[\"alpha\"] = alpha\n","    agent_weight = []\n","\n","    average_episode_rewards = []\n","    for run in tqdm(range(num_runs)):\n","\n","        env.env_init(env_info)\n","        agent.agent_init(agent_info)\n","\n","        reward_sums = []\n","\n","        episode_rewards = []\n","\n","        for episode in range(num_episodes):\n","\n","            # Runs an episode while keeping track of visited states\n","\n","            observation = env.env_start()\n","            action = agent.agent_start(observation)\n","\n","            single_reward = 0\n","\n","            is_terminal = False\n","            while True:\n","                reward, state, is_terminal = env.env_step(action)\n","                single_reward += reward\n","\n","                if is_terminal:\n","                    agent.agent_end(reward)\n","                    break\n","                action = agent.agent_step(reward, state)\n","\n","            episode_rewards.append(single_reward)\n","            env.env_cleanup()\n","        agent_weight.append(agent.w)\n","        average_episode_rewards.append(episode_rewards)\n","\n","    average_episode_rewards = np.mean(np.array(average_episode_rewards), 0)\n","    agent_weight = np.mean(np.array(agent_weight), 0)\n","    average_weight1.append(agent_weight)\n","\n","    agent_info2[\"alpha\"] = alpha\n","    agent2_weight = []\n","    average_episode_rewards2 = []\n","    for run in tqdm(range(num_runs)):\n","\n","        env.env_init(env_info)\n","        agent2.agent_init(agent_info2)\n","\n","        reward_sums = []\n","\n","        episode_rewards = []\n","\n","        for episode in range(num_episodes):\n","\n","            # Runs an episode while keeping track of visited states\n","\n","            observation = env.env_start()\n","            action = agent2.agent_start(observation)\n","\n","            single_reward = 0\n","\n","            is_terminal = False\n","            while True:\n","                reward, state, is_terminal = env.env_step(action)\n","                single_reward += reward\n","\n","                if is_terminal:\n","                    agent2.agent_end(reward)\n","                    break\n","                action = agent2.agent_step(reward, state)\n","\n","            episode_rewards.append(single_reward)\n","            env.env_cleanup()\n","        agent2_weight.append(agent2.w)\n","        average_episode_rewards2.append(episode_rewards)\n","\n","    average_episode_rewards2 = np.mean(np.array(average_episode_rewards2), 0)\n","    agent2_weight = np.mean(np.array(agent2_weight), 0)\n","    average_weight2.append(agent2_weight)\n","\n","    plt.plot(average_episode_rewards, label=\"sarsa\")\n","    plt.plot(average_episode_rewards2, label=\"q-learning\")\n","    plt.xlabel(\"Episodes\")\n","    plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\\n with\\n alpha\\n\" + str(alpha), rotation=0, labelpad=40)\n","    plt.xlim(0, num_episodes)\n","    plt.ylim(0, 1)\n","    plt.legend()\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBaEc4NjXlfw"},"source":["print(average_weight1)\n","print(average_weight2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3Pz98SyrYyn"},"source":["## run 1x5 grid world double q"]},{"cell_type":"code","metadata":{"id":"EnXNZQicrhTa"},"source":["from tqdm import tqdm\n","\n","# all_reward_sums = {}  # Contains sum of rewards during episode\n","# all_state_visits = {}  # Contains state visit counts during the last 10 episodes\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.01, \"gamma\": 1, \"alpha\": 0.01, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100  # The number of runs\n","num_episodes = 50000  # The number of episodes in each run\n","\n","env = SmallGridworld()\n","\n","agent = DoubleQLAgent()\n","\n","average_weight1 = []\n","\n","# for alpha in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]:\n","for alpha in [0.1]:\n","\n","    agent_info[\"alpha\"] = alpha\n","    agent_weight = []\n","\n","    average_episode_rewards = []\n","    for run in tqdm(range(num_runs)):\n","\n","        env.env_init(env_info)\n","        agent.agent_init(agent_info)\n","\n","        reward_sums = []\n","\n","        episode_rewards = []\n","\n","        for episode in range(num_episodes):\n","\n","            # Runs an episode while keeping track of visited states\n","\n","            observation = env.env_start()\n","            action = agent.agent_start(observation)\n","\n","            single_reward = 0\n","\n","            is_terminal = False\n","            while True:\n","                reward, state, is_terminal = env.env_step(action)\n","                single_reward += reward\n","\n","                if is_terminal:\n","                    agent.agent_end(reward)\n","                    break\n","                action = agent.agent_step(reward, state)\n","\n","            episode_rewards.append(single_reward)\n","            env.env_cleanup()\n","        agent_weight.append(agent.w1)\n","        agent_weight.append(agent.w2)\n","        average_episode_rewards.append(episode_rewards)\n","\n","    average_episode_rewards = np.mean(np.array(average_episode_rewards), 0)\n","    agent_weight = np.mean(np.array(agent_weight), 0)\n","    average_weight1.append(agent_weight)\n","\n","    plt.plot(average_episode_rewards, label=\"double q\")\n","    plt.xlabel(\"Episodes\")\n","    plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\\n\", rotation=0, labelpad=40)\n","    plt.xlim(0, num_episodes)\n","    plt.ylim(0, 1)\n","    plt.legend()\n","    plt.show()\n","  \n","print(\"done\")\n","print(average_weight1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o2tmo1a9p9Id"},"source":["## run 1 x 5 grid world dyna q"]},{"cell_type":"code","metadata":{"id":"tr7QlH6_qMdS"},"source":["from tqdm import tqdm\n","\n","# all_reward_sums = {}  # Contains sum of rewards during episode\n","# all_state_visits = {}  # Contains state visit counts during the last 10 episodes\n","env_info = {\"grid_height\": 1, \"grid_width\": 5}\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.01, \"gamma\": 1, \"alpha\": 0.00001, \"initial_weights\": 0.0,\n","              \"num_states\": 4, \"num_actions\": 2,\n","              \"dimensions\": 2, \"planning_steps\": 20, \"terminal_state\": 4, \"features\": np.array(\n","        [[[0, 1], [0.8, 0]], [[0, 0], [0.8, 0]], [[0, 0], [-1, 0]], [[0, 1], [-1, 0]]])}\n","\n","num_runs = 100  # The number of runs\n","num_episodes = 50000 # The number of episodes in each run\n","\n","env = SmallGridworld()\n","\n","agent = DynaQAgent()\n","\n","average_weight1 = []\n","\n","agent_weight = []\n","\n","average_episode_rewards = []\n","for run in tqdm(range(num_runs)):\n","\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    reward_sums = []\n","\n","    episode_rewards = []\n","\n","    for episode in range(num_episodes):\n","\n","        # Runs an episode while keeping track of visited states\n","\n","        observation = env.env_start()\n","        action = agent.agent_start(observation)\n","\n","        single_reward = 0\n","\n","        is_terminal = False\n","        while True:\n","            reward, state, is_terminal = env.env_step(action)\n","            single_reward += reward\n","\n","            if is_terminal:\n","                agent.agent_end(reward)\n","                break\n","            action = agent.agent_step(reward, state)\n","\n","        episode_rewards.append(single_reward)\n","        env.env_cleanup()\n","    agent_weight.append(agent.w)\n","    average_episode_rewards.append(episode_rewards)\n","\n","average_episode_rewards = np.mean(np.array(average_episode_rewards), 0)\n","agent_weight = np.mean(np.array(agent_weight), 0)\n","average_weight1.append(agent_weight)\n","\n","plt.plot(average_episode_rewards, label=\"dyna\")\n","plt.xlabel(\"Episodes\")\n","plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\\n\", rotation=0, labelpad=40)\n","plt.xlim(0, num_episodes)\n","plt.ylim(0, 1)\n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzwNqGQqr-Fe"},"source":["print(agent.w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNfF5Vy7LGPw"},"source":["## run 6 x 6 grid world q"]},{"cell_type":"code","metadata":{"id":"vciv-6IOLQLH"},"source":["from tqdm import tqdm\n","num_runs = 30  # The number of runs\n","num_episodes = 50000  # The number of episodes in each run\n","length_episode = 60\n","max_alpha = 0.1\n","min_alpha = 0.0001\n","\n","env_info = {\"grid_height\": 6, \"grid_width\": 6}\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.3, \"gamma\": 0.95, \"alpha\": max_alpha, \"initial_weights\": 0.0,\n","              \"num_states\": 6 * 6, \"num_actions\": 4,\n","              \"dimensions\": 6, \"is_testing\": False, \"features\": np.random.randn(6 * 6, 4, 6)}\n","\n","\n","def training(env, agent):\n","    agent.is_testing = False\n","\n","    episode_rewards = []\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","def testing(env, agent):\n","    agent.is_testing = True\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","env = GridWorldEnvironment()\n","agent = QLAgent()\n","\n","run_reward = []\n","\n","for run in tqdm(range(num_runs)):\n","\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    episode_rewards = []\n","\n","    for episode in range(num_episodes):\n","        training_reward = training(env, agent)\n","        testing_reward = testing(env, agent)\n","        agent.alpha -= (max_alpha - min_alpha) / num_episodes\n","\n","        episode_rewards.append(testing_reward)\n","\n","        # print(testing_reward)\n","    run_reward.append(episode_rewards)\n","\n","run_reward = np.mean(np.array(run_reward), 0)\n","\n","# print(run_reward)\n","\n","plt.plot(run_reward, label=\"q-learning\")\n","plt.xlabel(\"Episodes\")\n","plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\", rotation=0, labelpad=40)\n","plt.xlim(0, num_episodes)\n","plt.ylim(0, 1)\n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6JCdyFhYO0GA"},"source":["print(agent.w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_l5cEbvXiNk"},"source":["## run 6 x 6 grid world sarsa"]},{"cell_type":"code","metadata":{"id":"dCnXSM3yXqHB"},"source":["from tqdm import tqdm\n","num_runs = 30  # The number of runs\n","num_episodes = 50000  # The number of episodes in each run\n","length_episode = 60\n","max_alpha = 0.1\n","min_alpha = 0.0001\n","\n","env_info = {\"grid_height\": 6, \"grid_width\": 6}\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.3, \"gamma\": 0.95, \"alpha\": max_alpha, \"initial_weights\": 0.0,\n","              \"num_states\": 6 * 6, \"num_actions\": 4,\n","              \"dimensions\": 6, \"is_testing\": False, \"features\": np.random.randn(6 * 6, 4, 6)}\n","\n","\n","def training(env, agent):\n","    agent.is_testing = False\n","\n","    episode_rewards = []\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","def testing(env, agent):\n","    agent.is_testing = True\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","env = GridWorldEnvironment()\n","agent = SarsaAgent()\n","\n","run_reward = []\n","\n","for run in tqdm(range(num_runs)):\n","\n","    env.env_init(env_info)\n","    agent.agent_init(agent_info)\n","\n","    episode_rewards = []\n","\n","    for episode in range(num_episodes):\n","        training_reward = training(env, agent)\n","        testing_reward = testing(env, agent)\n","        agent.alpha -= (max_alpha - min_alpha) / num_episodes\n","\n","        episode_rewards.append(testing_reward)\n","\n","        # print(testing_reward)\n","    run_reward.append(episode_rewards)\n","\n","run_reward = np.mean(np.array(run_reward), 0)\n","\n","# print(run_reward)\n","\n","plt.plot(run_reward, label=\"q-learning\")\n","plt.xlabel(\"Episodes\")\n","plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\", rotation=0, labelpad=40)\n","plt.xlim(0, num_episodes)\n","plt.ylim(0, 1)\n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4lVC6t6ZfbM"},"source":["print(agent.w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TN8unUdJ6D6y"},"source":["## run 6 x 6 gridworld ExSarsa, Behaviour Epsilon 0.7"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"id":"AHUSoP6z6-0b","executionInfo":{"elapsed":2044,"status":"error","timestamp":1607541876015,"user":{"displayName":"Mehrad Mehrabi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjA7uP-luNxN6YOH-dY_0osdX55Glns9ewklIsZ=s64","userId":"15087250035822414101"},"user_tz":-210},"outputId":"047ee730-9d43-466e-a60d-96fb8f02f611"},"source":["from tqdm import tqdm\n","\n","num_runs = 10  # The number of runs\n","num_iterations = 100  # The number of episodes in each run\n","length_episode = 60\n","max_alpha = 0.1\n","min_alpha = 0.0001\n","\n","env_info = {\"grid_height\": 6, \"grid_width\": 6}\n","agent_info = {\"epsilon\": 0.7, \"target_epsilon\": 0.7, \"gamma\": 0.95, \"alpha\": max_alpha, \"initial_weights\": 0.0,\n","              \"num_states\": 6 * 6, \"num_actions\": 4,\n","              \"dimensions\": 6, \"is_testing\": False,\n","              \"planning_steps\": 10, \"terminal_state\": -1, \"features\": np.random.randn(6 * 6, 4, 6)}\n","\n","\n","def training(env, agent):\n","    agent.is_testing = False\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","def testing(env, agent):\n","    agent.is_testing = True\n","\n","    observation = env.env_start()\n","    action = agent.agent_start(observation)\n","\n","    single_reward = 0\n","\n","    for i in range(length_episode):\n","        reward, state, is_terminal = env.env_step(action)\n","        single_reward += reward\n","\n","        action = agent.agent_step(reward, state)\n","\n","    env.env_cleanup()\n","\n","    return single_reward\n","\n","\n","env = GridWorldEnvironment()\n","# agent = QLAgent()\n","\n","alphas_dic = [\"0.1\":0.1, \"0.2\":0.2, \"0.3\":0.3, \"0.4\":0.4, \"0.5\":0.5, \"0.6\":0.6,\n","              \"0.7\":0.7, \"0.8\":0.8]\n","\n","\n","for alpha in alphas_dic:\n","    run_reward = []\n","    for run in tqdm(range(num_runs)):\n","\n","        env.env_init(env_info)\n","        agent = ExAgent()\n","        agent.agent_init(agent_info)\n","        agent.alpha = alpha\n","        episode_rewards = []\n","\n","        for episode in range(num_episodes):\n","            #training_reward = training(env, agent_dic[agent_name])\n","            testing_reward = testing(env, agent)\n","            # agent.alpha -= (max_alpha - min_alpha) / num_episodes\n","\n","            episode_rewards.append(testing_reward)\n","            # print(training_reward)\n","            # if testing_reward != 0:\n","            #     print(testing_reward)\n","\n","        run_reward.append(episode_rewards)\n","\n","    run_reward = np.mean(np.array(run_reward), 0)\n","\n","    np.save(agent_name + \".npy\", run_reward)\n","\n","    string_name = agent_name.replace(\"_\", \" \")\n","    plt.plot(run_reward, label=string_name)\n","plt.xlabel(\"Episodes\")\n","plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\", rotation=0, labelpad=40)\n","plt.xlim(0, num_episodes)\n","plt.ylim(0, 300)\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-eb61ce49389b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridWorldEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExagent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mrun_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Exagent' is not defined"]}]}]}